{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef8ad6d0",
   "metadata": {
    "id": "ef8ad6d0",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0684c3b",
   "metadata": {
    "id": "f0684c3b"
   },
   "source": [
    "- git lfs install\n",
    "- git clone https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "VgEwrmRi9R9v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4014,
     "status": "ok",
     "timestamp": 1646867647468,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "VgEwrmRi9R9v",
    "outputId": "ba559454-8870-4081-9e08-dea20c2fe29e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated git hooks.\n",
      "Git LFS initialized.\n",
      "fatal: destination path 'distilroberta-finetuned-financial-news-sentiment-analysis' already exists and is not an empty directory.\n",
      "Requirement already satisfied: transformers in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (4.16.2)\n",
      "Requirement already satisfied: sacremoses in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: six in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /Users/TyPainter1/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bq54txr8zqn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1646867648477,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "1bq54txr8zqn",
    "outputId": "45e772fa-608f-4a7d-e970-9a950ac76cc7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "import os\n",
    "# cur_path = \"/content/drive/MyDrive/Colab Notebooks/capstone/\"\n",
    "# os.chdir(cur_path)\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00c4037f",
   "metadata": {
    "executionInfo": {
     "elapsed": 15544,
     "status": "ok",
     "timestamp": 1646867663988,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "00c4037f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d728f676",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1646867663991,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "d728f676"
   },
   "outputs": [],
   "source": [
    "def load_data(): \n",
    "    path = r'/Users/TyPainter1/Desktop/Masters/spring-2022/capstone/00-data/gdelt_data/'\n",
    "    #path = r'/content/drive/MyDrive/Colab Notebooks/capstone/gdelt_data/'\n",
    "    years = os.listdir(path)\n",
    "    years.remove('.DS_Store')\n",
    "    years.remove('.ipynb_checkpoints')\n",
    "    yr = [x for x in years if not (x.startswith('.'))]\n",
    "\n",
    "    year_paths= []\n",
    "    months=[]\n",
    "    file_paths= []\n",
    "\n",
    "    for i in range(len(years)): # enter year folder\n",
    "        year_paths.append(path + years[i]) # year paths\n",
    "        months.extend(os.listdir(year_paths[i])) # months of data in each year\n",
    "    months = [x for x in months if not (x.startswith('.'))]\n",
    "    for j in range(len(months)): # enter months in each year folder\n",
    "        file_paths.append(year_paths[i] + \"/\" + months[j]) # file in year path\n",
    "    \n",
    "    return(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca7b3071",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1646867663992,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "ca7b3071"
   },
   "outputs": [],
   "source": [
    "def filter_websites(file):\n",
    "    df = pd.read_csv(file, index_col=[0]) # read in csv\n",
    "    m_df = df[df.website=='marketwatch.com'] # filter for website\n",
    "    p_df = df[df.website=='prnewswire.com'] # filter for website\n",
    "    y_df = df[df.website=='yahoo.com'][0:10] # filter for website\n",
    "    \n",
    "    return(m_df, p_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4ce3ffa",
   "metadata": {
    "executionInfo": {
     "elapsed": 2436,
     "status": "ok",
     "timestamp": 1646868009935,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "d4ce3ffa"
   },
   "outputs": [],
   "source": [
    "roberta = pipeline(task='sentiment-analysis', \n",
    "                   model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\",\n",
    "                   return_all_scores=True,\n",
    "                   max_length=514, \n",
    "                   truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bc9868a",
   "metadata": {
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1646867679038,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "0bc9868a"
   },
   "outputs": [],
   "source": [
    "def summary_stats(score):\n",
    "    avg = np.mean(score)\n",
    "    med = np.median(score)\n",
    "    min = np.min(score)\n",
    "    max = np.max(score)\n",
    "    return(avg, med, min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b26ac37",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646867679039,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "9b26ac37"
   },
   "outputs": [],
   "source": [
    "# yahoo web scrape NLP\n",
    "def yahoo_nlp(urls, df):\n",
    "\n",
    "    df[\"pos_mean\"] = np.nan\n",
    "    df[\"pos_median\"] = np.nan\n",
    "    df[\"pos_min\"] = np.nan\n",
    "    df[\"pos_max\"] = np.nan\n",
    "\n",
    "    df[\"neg_mean\"] = np.nan\n",
    "    df[\"neg_median\"] = np.nan\n",
    "    df[\"neg_min\"] = np.nan\n",
    "    df[\"neg_max\"] = np.nan\n",
    "\n",
    "    df[\"neu_mean\"] = np.nan\n",
    "    df[\"neu_median\"] = np.nan\n",
    "    df[\"neu_min\"] = np.nan\n",
    "    df[\"neu_max\"] = np.nan\n",
    "    \n",
    "    nf = \"Content is currently unavailable\"\n",
    "    \n",
    "    for i in range(len(urls)): #filter_urls\n",
    "        URL = urls.iloc[i] # filter_urls\n",
    "        page = requests.get(URL,headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'})\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        if nf in soup.prettify():\n",
    "                continue\n",
    "        try:\n",
    "            try:\n",
    "                master = soup.find(id=\"Masterwrap\")\n",
    "                final = master.find(\"div\", class_=\"caas-body\")\n",
    "            except AttributeError:\n",
    "                master = soup.find(id=\"Masterwrap2Col\")\n",
    "                final = master.find(\"div\", class_=\"caas-body\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        text = re.sub('<[^>]+>', '', str(final))\n",
    "        sen_list = text.split(\".\")\n",
    "\n",
    "        que = \"\"\n",
    "        char_len = 0\n",
    "        pos = []\n",
    "        neg = []\n",
    "        neu = []\n",
    "\n",
    "        for sen in sen_list:\n",
    "            if len(sen_list)-1 == 1: # article is only 1 sentence\n",
    "                que = sen\n",
    "                output = roberta(que)\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "            elif ((sen_list.index(sen) == len(sen_list)-1) & (char_len + len(sen) < 514)): # end of article and can combine\n",
    "                que = que + sen\n",
    "                char_len = char_len + len(sen)\n",
    "                output = roberta(que)\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "            elif ((sen_list.index(sen) == len(sen_list)-1) & (char_len + len(sen) >= 514)): # end of article and cannot combine\n",
    "                output = roberta(que) # run que\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "                que = sen # assign last sentence\n",
    "                char_len = len(sen)\n",
    "                output = roberta(que) # run last sentence\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "            elif char_len + len(sen) < 514: # combine sentences\n",
    "                que = que + sen\n",
    "                char_len = char_len + len(sen)\n",
    "            else:\n",
    "                output = roberta(que)\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "                que = \"\"\n",
    "                que = sen\n",
    "                char_len = len(sen)\n",
    "\n",
    "        df.pos_mean.iloc[i], df.pos_median.iloc[i], df.pos_min.iloc[i], df.pos_max.iloc[i] = summary_stats(pos)\n",
    "        df.neg_mean.iloc[i], df.neg_median.iloc[i], df.neg_min.iloc[i], df.neg_max.iloc[i] = summary_stats(neg)\n",
    "        df.neu_mean.iloc[i], df.neu_median.iloc[i], df.neu_min.iloc[i], df.neu_max.iloc[i] = summary_stats(neu)\n",
    "        \n",
    "    df = df.dropna()\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e7687c2",
   "metadata": {
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1646867679176,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "8e7687c2"
   },
   "outputs": [],
   "source": [
    "# marketwatch web scrape NLP\n",
    "def mwatch_nlp(urls, df):\n",
    "    \n",
    "    df[\"pos_mean\"] = np.nan\n",
    "    df[\"pos_median\"] = np.nan\n",
    "    df[\"pos_min\"] = np.nan\n",
    "    df[\"pos_max\"] = np.nan\n",
    "\n",
    "    df[\"neg_mean\"] = np.nan\n",
    "    df[\"neg_median\"] = np.nan\n",
    "    df[\"neg_min\"] = np.nan\n",
    "    df[\"neg_max\"] = np.nan\n",
    "\n",
    "    df[\"neu_mean\"] = np.nan\n",
    "    df[\"neu_median\"] = np.nan\n",
    "    df[\"neu_min\"] = np.nan\n",
    "    df[\"neu_max\"] = np.nan\n",
    "    \n",
    "    nf = \"Story not found\"\n",
    "    \n",
    "    for i in range(len(urls)): #filter_urls\n",
    "        URL = urls.iloc[i] # filter_urls\n",
    "        page = requests.get(URL,headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'})\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        if nf in soup.prettify():\n",
    "                continue\n",
    "        try:\n",
    "            master = soup.find(id=\"maincontent\")\n",
    "            final = master.find(\"div\", class_=\"article__body article-wrap at16-col16 barrons-article-wrap\")\n",
    "            text = final.text\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        sen_list = text.split(\".\")\n",
    "\n",
    "        que = \"\"\n",
    "        char_len = 0\n",
    "        pos = []\n",
    "        neg = []\n",
    "        neu = []\n",
    "\n",
    "        for sen in sen_list:\n",
    "            if len(sen_list)-1 == 1: # article is only 1 sentence\n",
    "                que = sen\n",
    "                output = roberta(que)\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "            elif ((sen_list.index(sen) == len(sen_list)-1) & (char_len + len(sen) < 514)): # end of article and can combine\n",
    "                que = que + sen\n",
    "                char_len = char_len + len(sen)\n",
    "                output = roberta(que)\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "            elif ((sen_list.index(sen) == len(sen_list)-1) & (char_len + len(sen) >= 514)): # end of article and cannot combine\n",
    "                output = roberta(que) # run que\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "                que = sen # assign last sentence\n",
    "                char_len = len(sen)\n",
    "                output = roberta(que) # run last sentence\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "            elif char_len + len(sen) < 514: # combine sentences\n",
    "                que = que + sen\n",
    "                char_len = char_len + len(sen)\n",
    "            else:\n",
    "                output = roberta(que)\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "                que = \"\"\n",
    "                que = sen\n",
    "                char_len = len(sen)\n",
    "\n",
    "        df.pos_mean.iloc[i], df.pos_median.iloc[i], df.pos_min.iloc[i], df.pos_max.iloc[i] = summary_stats(pos)\n",
    "        df.neg_mean.iloc[i], df.neg_median.iloc[i], df.neg_min.iloc[i], df.neg_max.iloc[i] = summary_stats(neg)\n",
    "        df.neu_mean.iloc[i], df.neu_median.iloc[i], df.neu_min.iloc[i], df.neu_max.iloc[i] = summary_stats(neu)\n",
    "        \n",
    "    df = df.dropna()\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70adfe37",
   "metadata": {
    "executionInfo": {
     "elapsed": 148,
     "status": "ok",
     "timestamp": 1646867679322,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "70adfe37"
   },
   "outputs": [],
   "source": [
    "# prnewswire web scrape NLP\n",
    "def prnw_nlp(urls, df):\n",
    "    \n",
    "    df[\"pos_mean\"] = np.nan\n",
    "    df[\"pos_median\"] = np.nan\n",
    "    df[\"pos_min\"] = np.nan\n",
    "    df[\"pos_max\"] = np.nan\n",
    "\n",
    "    df[\"neg_mean\"] = np.nan\n",
    "    df[\"neg_median\"] = np.nan\n",
    "    df[\"neg_min\"] = np.nan\n",
    "    df[\"neg_max\"] = np.nan\n",
    "\n",
    "    df[\"neu_mean\"] = np.nan\n",
    "    df[\"neu_median\"] = np.nan\n",
    "    df[\"neu_min\"] = np.nan\n",
    "    df[\"neu_max\"] = np.nan\n",
    "    \n",
    "    nf = \"Content is currently unavailable\"\n",
    "    \n",
    "    for i in range(len(urls)): #filter_urls\n",
    "        URL = urls.iloc[i] # filter_urls\n",
    "        page = requests.get(URL,headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'})\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        if nf in soup.prettify():\n",
    "                continue\n",
    "        try:\n",
    "            master = soup.find(id=\"main\")\n",
    "            final = master.find(\"div\", class_=\"col-sm-10 col-sm-offset-1\")\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        try:\n",
    "            text = final.text\n",
    "        except:\n",
    "            pass\n",
    "        text = re.sub('<[^>]+>', '', str(final))\n",
    "        sen_list = text.split(\".\")\n",
    "\n",
    "        que = \"\"\n",
    "        char_len = 0\n",
    "        pos = []\n",
    "        neg = []\n",
    "        neu = []\n",
    "\n",
    "        for sen in sen_list:\n",
    "            if len(sen_list)-1 == 1: # article is only 1 sentence\n",
    "                que = sen\n",
    "                output = roberta(que)\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "            elif ((sen_list.index(sen) == len(sen_list)-1) & (char_len + len(sen) < 514)): # end of article and can combine\n",
    "                que = que + sen\n",
    "                char_len = char_len + len(sen)\n",
    "                output = roberta(que)\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "            elif ((sen_list.index(sen) == len(sen_list)-1) & (char_len + len(sen) >= 514)): # end of article and cannot combine\n",
    "                output = roberta(que) # run que\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "                que = sen # assign last sentence\n",
    "                char_len = len(sen)\n",
    "                output = roberta(que) # run last sentence\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "            elif char_len + len(sen) < 514: # combine sentences\n",
    "                que = que + sen\n",
    "                char_len = char_len + len(sen)\n",
    "            else:\n",
    "                output = roberta(que)\n",
    "                neg.append(output[0][0]['score'])\n",
    "                neu.append(output[0][1]['score'])\n",
    "                pos.append(output[0][2]['score'])\n",
    "                que = \"\"\n",
    "                que = sen\n",
    "                char_len = len(sen)\n",
    "\n",
    "        df.pos_mean.iloc[i], df.pos_median.iloc[i], df.pos_min.iloc[i], df.pos_max.iloc[i] = summary_stats(pos)\n",
    "        df.neg_mean.iloc[i], df.neg_median.iloc[i], df.neg_min.iloc[i], df.neg_max.iloc[i] = summary_stats(neg)\n",
    "        df.neu_mean.iloc[i], df.neu_median.iloc[i], df.neu_min.iloc[i], df.neu_max.iloc[i] = summary_stats(neu)\n",
    "        \n",
    "    df = df.dropna()\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6740d292",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646867679323,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "6740d292"
   },
   "outputs": [],
   "source": [
    "# append yahoo, marketwatch, and prnewswire together for entire month dataset\n",
    "def append_df(df1,df2,df3):\n",
    "    df = df1.append([df2,df3])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85e4f752",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "executionInfo": {
     "elapsed": 7457,
     "status": "error",
     "timestamp": 1646868023711,
     "user": {
      "displayName": "Ty Painter",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04234853017868576860"
     },
     "user_tz": 360
    },
    "id": "85e4f752",
    "outputId": "d278ed94-ace6-4ae1-fb62-f2b20fef439b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_paths loaded\n",
      "/Users/TyPainter1/Desktop/Masters/spring-2022/capstone/00-data/gdelt_data/2021/gdelt_mar2021.csv\n",
      "filter websites\n",
      "URL\n",
      "mwatch NLP\n",
      "prnw NLP\n",
      "yahoo NLP\n",
      "Append\n",
      "/Users/TyPainter1/Desktop/Masters/spring-2022/capstone/00-data/gdelt_data/2021/gdelt_jan2021.csv\n",
      "filter websites\n",
      "URL\n",
      "mwatch NLP\n",
      "prnw NLP\n",
      "yahoo NLP\n",
      "Append\n",
      "Completed in 1.78 minutes\n"
     ]
    }
   ],
   "source": [
    "tic = time.perf_counter()\n",
    "file_paths = load_data()\n",
    "print(\"file_paths loaded\")\n",
    "for path in file_paths:\n",
    "    print(path)\n",
    "    m_df, p_df, y_df = filter_websites(path)\n",
    "    print(\"filter websites\")\n",
    "    m_df = m_df[0:10]\n",
    "    p_df = p_df[0:10]\n",
    "    y_df = y_df[0:10]\n",
    "    \n",
    "    m_url = m_df.url\n",
    "    p_url = p_df.url\n",
    "    y_url = y_df.url\n",
    "    print(\"URL\")\n",
    "    \n",
    "    mwatch_df = mwatch_nlp(m_url, m_df)\n",
    "    print(\"mwatch NLP\")\n",
    "    prnw_df = prnw_nlp(p_url, p_df)\n",
    "    print(\"prnw NLP\")\n",
    "    yahoo_df = yahoo_nlp(y_url, y_df)\n",
    "    print(\"yahoo NLP\")\n",
    "    \n",
    "    month_df = append_df(yahoo_df,mwatch_df, prnw_df)\n",
    "    print(\"Append\")\n",
    "    \n",
    "    year_number = str(month_df.year.unique().item())\n",
    "    month_number = str(month_df.month.unique().item())\n",
    "    datetime_object = datetime.datetime.strptime(month_number, \"%m\")\n",
    "    month_name = datetime_object.strftime(\"%b\").lower()\n",
    "    file_name = 'nlp_'+month_name+str(year_number)+'.csv'\n",
    "    month_df.to_csv(file_name,\n",
    "                   index=False)\n",
    "    \n",
    "toc = time.perf_counter()\n",
    "print(f\"Completed in {(toc - tic)/60:0.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlp_sentiment_scores.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
