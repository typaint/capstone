# -*- coding: utf-8 -*-
"""nlp_sentiment_scores-Copy1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cP5D6WFxVcneS-aH6TPQrSuP0lStzmWh

<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#Data" data-toc-modified-id="Data-1">Data</a></span><ul class="toc-item"><li><span><a href="#Load-Data" data-toc-modified-id="Load-Data-1.1">Load Data</a></span></li><li><span><a href="#Filter-Data" data-toc-modified-id="Filter-Data-1.2">Filter Data</a></span></li></ul></li><li><span><a href="#Readability" data-toc-modified-id="Readability-2">Readability</a></span></li><li><span><a href="#Tag-Articles" data-toc-modified-id="Tag-Articles-3">Tag Articles</a></span></li><li><span><a href="#NLP" data-toc-modified-id="NLP-4">NLP</a></span><ul class="toc-item"><li><span><a href="#Load-Roberta-Pipeline" data-toc-modified-id="Load-Roberta-Pipeline-4.1">Load Roberta Pipeline</a></span></li><li><span><a href="#Summary-Stats" data-toc-modified-id="Summary-Stats-4.2">Summary Stats</a></span></li><li><span><a href="#Yahoo" data-toc-modified-id="Yahoo-4.3">Yahoo</a></span></li><li><span><a href="#MarketWatch" data-toc-modified-id="MarketWatch-4.4">MarketWatch</a></span></li><li><span><a href="#PR-Newswire" data-toc-modified-id="PR-Newswire-4.5">PR Newswire</a></span></li></ul></li><li><span><a href="#Example" data-toc-modified-id="Example-5">Example</a></span></li></ul></div>

- git lfs install
- git clone https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis
"""

# !git lfs install
# !git clone https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis
# !pip install transformers

# from google.colab import drive
# drive.mount('/content/drive')
import os
# cur_path = "/content/drive/MyDrive/Colab Notebooks/capstone/"
# os.chdir(cur_path)
# !pwd

import pandas as pd
from bs4 import BeautifulSoup
import requests
import re
import numpy as np
from transformers import pipeline
import time
import datetime

"""## Data

### Load Data
"""

def load_data():
    #path = r'/Users/TyPainter1/Desktop/Masters/spring-2022/capstone/00-data/gdelt_data/'
    path = r'../00-data/gdelt_data/'
    years = os.listdir(path)
    if '.DS_Store' in years:
      years.remove('.DS_Store')
    if '.ipynb_checkpoints' in years:
      years.remove('.ipynb_checkpoints')
    yr = [x for x in years if not (x.startswith('.'))]

    year_paths= []
    months=[]
    file_paths= []

    for i in range(len(years)): # enter year folder
        year_paths.append(path + years[i]) # year paths
        months.extend(os.listdir(year_paths[i])) # months of data in each year
    months = [x for x in months if not (x.startswith('.'))]
    for j in range(len(months)): # enter months in each year folder
        file_paths.append(year_paths[i] + "/" + months[j]) # file in year path

    return(file_paths)

"""### Filter Data"""

def filter_websites(file):
    df = pd.read_csv(file, index_col=[0]) # read in csv
    m_df = df[df.website=='marketwatch.com'] # filter for website
    p_df = df[df.website=='prnewswire.com'] # filter for website
    y_df = df[df.website=='yahoo.com'] # filter for website

    return(m_df, p_df, y_df)

"""## Readability"""

#  !pip install https://github.com/andreasvc/readability/tarball/master

import readability

def read_grades(article):
    if article == "NA":
        results = readability.getmeasures(article, lang='en')
        cols = list(results['readability grades'])
        vals = np.empty((1,9))
        vals[:] = np.nan
        scores = pd.DataFrame(data=vals,columns=cols)
    elif article == "":
        article = "NA"
        results = readability.getmeasures(article, lang='en')
        cols = list(results['readability grades'])
        vals = np.empty((1,9))
        vals[:] = np.nan
        scores = pd.DataFrame(data=vals,columns=cols)
    else:
        results = readability.getmeasures(article, lang='en')
        cols = list(results['readability grades'])
        vals = list(results['readability grades'].values())
        vals =[[round(val,2)] for val in vals]
        vals = np.asarray(vals).T
        scores = pd.DataFrame(data=vals,columns=cols)
    return(scores)

def sentence_info(article):
    if article == "NA":
        results = readability.getmeasures(article, lang='en')
        cols = list(results['sentence info'])
        idx = [7,-2]
        cols = [cols[i] for i in idx]
        vals = np.empty((1,2))
        vals[:] = np.nan
        scores = pd.DataFrame(data=vals,columns=cols)
    elif article == "":
        article = "NA"
        results = readability.getmeasures(article, lang='en')
        cols = list(results['sentence info'])
        idx = [7,-2]
        cols = [cols[i] for i in idx]
        vals = np.empty((1,2))
        vals[:] = np.nan
        scores = pd.DataFrame(data=vals,columns=cols)
    else:
        results = readability.getmeasures(article, lang='en')
        cols = list(results['sentence info'])
        idx = [7,-2]
        cols = [cols[i] for i in idx]
        vals = list(results['sentence info'].values())
        vals = [[vals[i]] for i in idx]
        vals = np.asarray(vals).T
        scores = pd.DataFrame(data=vals,columns=cols)
    return(scores)

"""## Tag Articles"""

# !pip install fuzzywuzzy[speedup]

from fuzzywuzzy import fuzz
from fuzzywuzzy import process

tick_comp_df = pd.read_csv("../00-data/company_tickers.csv")
tick_comp_df.head()

companies = tick_comp_df["Security"].to_list()
companies

def tag_article(article, companies, df): #, tickers
    if (article == "NA") | (article==""):
        tags = np.nan
    else:
        scores = process.extract(article,
                                 companies,
                                 scorer=fuzz.token_set_ratio,
                                 limit=50)
        comp_tags = [scores[i][0] for i in range(len(scores)) if scores[i][1]>90]
        tags = list(df.loc[df.Security.isin(comp_tags), "Symbol"])
        if len(tags) == 0:
            tags = np.nan
    return(tags)

def row_tags(old_df):
    old_df = old_df.reset_index(drop=True)
    new_df = pd.DataFrame(columns=old_df.columns)
    for i in range(len(old_df)):
        new_tags = old_df.tags[i]#.strip("][").replace("'","").split(', ')
        for t in new_tags:
            new_df = new_df.append(old_df.iloc[i,:-1]).reset_index(drop=True)
            new_df.tags.iloc[-1] = t
    return(new_df)

"""## NLP

### Load Roberta Pipeline
"""

roberta = pipeline(task='sentiment-analysis',
                   model="mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis",
                   return_all_scores=True,
                   max_length=514,
                   truncation=True)

""" ### Summary Stats"""

def summary_stats(score):
    if len(score)== 0:
        avg = np.nan
        med = np.nan
        min = np.nan
        max = np.nan
    else:
        avg = round(np.mean(score),2)
        med = round(np.median(score),2)
        min = round(np.min(score),2)
        max = round(np.max(score),2)
    return(avg, med, min, max)

"""### Yahoo"""

# yahoo web scrape NLP
def yahoo_nlp(urls, df):

    df["pos_mean"] = np.nan
    df["pos_median"] = np.nan
    df["pos_min"] = np.nan
    df["pos_max"] = np.nan

    df["neg_mean"] = np.nan
    df["neg_median"] = np.nan
    df["neg_min"] = np.nan
    df["neg_max"] = np.nan

    df["neu_mean"] = np.nan
    df["neu_median"] = np.nan
    df["neu_min"] = np.nan
    df["neu_max"] = np.nan

    df["net_mean"] = np.nan
    df["net_median"] = np.nan
    df["net_min"] = np.nan
    df["net_max"] = np.nan

    score_df = pd.DataFrame()
    sentence_df = pd.DataFrame()
    tag_df = pd.DataFrame(columns=["tags"])

    nf = "Content is currently unavailable"

    for i in range(len(urls)): #filter_urls
        URL = urls.iloc[i] # filter_urls
        page = requests.get(URL,headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'})
        soup = BeautifulSoup(page.content, "html.parser")
        if nf in soup.prettify():
            text = "NA"
            score_df = score_df.append(read_grades(text))
            sentence_df = sentence_df.append(sentence_info(text))
            tag_df.at[i, 'tags'] = tag_article(text,companies,tick_comp_df)

            continue
        try:
            try:
                master = soup.find(id="Masterwrap")
                final = master.find("div", class_="caas-body")
            except AttributeError:
                master = soup.find(id="Masterwrap2Col")
                final = master.find("div", class_="caas-body")
        except AttributeError:
            pass

        text = re.sub('(<p>)', ' ', str(final))
        text = re.sub('<[^>]+>', '', str(text))
        sen_list = text.split(". ")[:-2]
        article = [".\n ".join(sen_list[:-2])][0] # remove author signature
        score_df = score_df.append(read_grades(article))
        sentence_df = sentence_df.append(sentence_info(article))
        tag_df.at[i, 'tags'] = tag_article(article,companies,tick_comp_df)

        pos = []
        neg = []
        neu = []
        net = []

        for sen in sen_list:
            try:
                output = roberta(sen)
            except IndexError:
                output = roberta(sen[0:514])
            neg.append(output[0][0]['score'])
            neu.append(output[0][1]['score'])
            pos.append(output[0][2]['score'])
            net.append(pos[-1] - neg[-1])

        df.pos_mean.iloc[i], df.pos_median.iloc[i], df.pos_min.iloc[i], df.pos_max.iloc[i] = summary_stats(pos)
        df.neg_mean.iloc[i], df.neg_median.iloc[i], df.neg_min.iloc[i], df.neg_max.iloc[i] = summary_stats(neg)
        df.neu_mean.iloc[i], df.neu_median.iloc[i], df.neu_min.iloc[i], df.neu_max.iloc[i] = summary_stats(neu)
        df.net_mean.iloc[i], df.net_median.iloc[i], df.net_min.iloc[i], df.net_max.iloc[i] = summary_stats(net)

        if i%50==0:
          print("y: " + str(i)+"/"+str(len(urls)))

    score_df.index = df.index
    sentence_df.index = df.index
    tag_df.index = df.index
    df = pd.concat([df,score_df,sentence_df,tag_df],axis=1)
    df = df.dropna()

    return(df)

"""### MarketWatch"""

# marketwatch web scrape NLP
def mwatch_nlp(urls, df):

    df["pos_mean"] = np.nan
    df["pos_median"] = np.nan
    df["pos_min"] = np.nan
    df["pos_max"] = np.nan

    df["neg_mean"] = np.nan
    df["neg_median"] = np.nan
    df["neg_min"] = np.nan
    df["neg_max"] = np.nan

    df["neu_mean"] = np.nan
    df["neu_median"] = np.nan
    df["neu_min"] = np.nan
    df["neu_max"] = np.nan

    df["net_mean"] = np.nan
    df["net_median"] = np.nan
    df["net_min"] = np.nan
    df["net_max"] = np.nan

    score_df = pd.DataFrame()
    sentence_df = pd.DataFrame()
    tag_df = pd.DataFrame(columns=["tags"])

    nf = "Story not found"

    for i in range(len(urls)): #filter_urls
        URL = urls.iloc[i] # filter_urls
        page = requests.get(URL,headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'})
        soup = BeautifulSoup(page.content, "html.parser")
        if nf in soup.prettify():
            text = "NA"
            score_df = score_df.append(read_grades(text))
            sentence_df = sentence_df.append(sentence_info(text))
            tag_df.at[i, 'tags'] = tag_article(text,companies,tick_comp_df)
            continue
        try:
            master = soup.find(id="maincontent")
            final = master.find("div", class_="article__body article-wrap at16-col16 barrons-article-wrap")
            text = final.text.strip()
        except AttributeError:
            pass
        sen_list = text.split(".")
        article = [".\n ".join(sen_list)][0]
        score_df = score_df.append(read_grades(article))
        sentence_df = sentence_df.append(sentence_info(article))
        tag_df.at[i, 'tags'] = tag_article(article,companies,tick_comp_df)

        pos = []
        neg = []
        neu = []
        net = []

        for sen in sen_list:
            try:
                output = roberta(sen)
            except IndexError:
                output = roberta(sen[0:514])
            neg.append(output[0][0]['score'])
            neu.append(output[0][1]['score'])
            pos.append(output[0][2]['score'])
            net.append(pos[-1] - neg[-1])

        df.pos_mean.iloc[i], df.pos_median.iloc[i], df.pos_min.iloc[i], df.pos_max.iloc[i] = summary_stats(pos)
        df.neg_mean.iloc[i], df.neg_median.iloc[i], df.neg_min.iloc[i], df.neg_max.iloc[i] = summary_stats(neg)
        df.neu_mean.iloc[i], df.neu_median.iloc[i], df.neu_min.iloc[i], df.neu_max.iloc[i] = summary_stats(neu)
        df.net_mean.iloc[i], df.net_median.iloc[i], df.net_min.iloc[i], df.net_max.iloc[i] = summary_stats(net)

        if i%50==0:
          print("mw: " + str(i)+"/"+str(len(urls)))

    score_df.index = df.index
    sentence_df.index = df.index
    tag_df.index = df.index
    df = pd.concat([df,score_df,sentence_df,tag_df],axis=1)
    df = df.dropna()

    return(df)

"""### PR Newswire"""

# prnewswire web scrape NLP
def prnw_nlp(urls, df):

    df["pos_mean"] = np.nan
    df["pos_median"] = np.nan
    df["pos_min"] = np.nan
    df["pos_max"] = np.nan

    df["neg_mean"] = np.nan
    df["neg_median"] = np.nan
    df["neg_min"] = np.nan
    df["neg_max"] = np.nan

    df["neu_mean"] = np.nan
    df["neu_median"] = np.nan
    df["neu_min"] = np.nan
    df["neu_max"] = np.nan

    df["net_mean"] = np.nan
    df["net_median"] = np.nan
    df["net_min"] = np.nan
    df["net_max"] = np.nan

    sentence_df = pd.DataFrame()
    score_df = pd.DataFrame()
    tag_df = pd.DataFrame(columns=["tags"])

    nf = "Content is currently unavailable"

    for i in range(len(urls)): #filter_urls
        URL = urls.iloc[i] # filter_urls
        page = requests.get(URL,headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'})
        soup = BeautifulSoup(page.content, "html.parser")
        if nf in soup.prettify():
            text = "NA"
            score_df = score_df.append(read_grades(text))
            sentence_df = sentence_df.append(sentence_info(text))
            tag_df.at[i, 'tags'] = tag_article(text,companies,tick_comp_df)
            continue
        try:
            master = soup.find(id="main")
            final = master.find_all("div", class_="col-sm-10 col-sm-offset-1")
        except AttributeError:
            pass
        try:
            paragraphs = []
            for wrapper in final:
                paragraphs.append(wrapper.text.strip())
                text = [" ".join(paragraphs)][0]
        except:
            pass
        sen_list = text.split(". ")
        article = [".\n ".join(sen_list)][0]
        score_df = score_df.append(read_grades(article))
        sentence_df = sentence_df.append(sentence_info(article))
        tag_df.at[i, 'tags'] = tag_article(article,companies,tick_comp_df)

        pos = []
        neg = []
        neu = []
        net = []

        for sen in sen_list:
            try:
                output = roberta(sen)
            except IndexError:
                output = roberta(sen[0:514])
            neg.append(output[0][0]['score'])
            neu.append(output[0][1]['score'])
            pos.append(output[0][2]['score'])
            net.append(pos[-1] - neg[-1])
        df.pos_mean.iloc[i], df.pos_median.iloc[i], df.pos_min.iloc[i], df.pos_max.iloc[i] = summary_stats(pos)
        df.neg_mean.iloc[i], df.neg_median.iloc[i], df.neg_min.iloc[i], df.neg_max.iloc[i] = summary_stats(neg)
        df.neu_mean.iloc[i], df.neu_median.iloc[i], df.neu_min.iloc[i], df.neu_max.iloc[i] = summary_stats(neu)
        df.net_mean.iloc[i], df.net_median.iloc[i], df.net_min.iloc[i], df.net_max.iloc[i] = summary_stats(net)

        if i%50==0:
          print("pr: " + str(i)+"/"+str(len(urls)))

    score_df.index = df.index
    sentence_df.index = df.index
    tag_df.index = df.index
    df = pd.concat([df,score_df,sentence_df,tag_df],axis=1)
    df = df.dropna()

    return(df)

"""## Example"""

# append yahoo, marketwatch, and prnewswire together for entire month dataset
def append_df(df1,df2,df3):
    df = df1.append([df2,df3])
    return(df)

tic = time.perf_counter()
#file_paths = load_data()
file_paths = ['../00-data/gdelt_data/2021/gdelt_feb2021.csv']
print("file_paths loaded")
print(file_paths)
for path in file_paths:
    print(path)
    m_df, p_df, y_df = filter_websites(path)
    print("filter websites")
    m_df = m_df
    p_df = p_df
    y_df = y_df

    m_url = m_df.url
    p_url = p_df.url
    y_url = y_df.url
    print("URL")

    mwatch_df = mwatch_nlp(m_url, m_df)
    print("mwatch NLP")
    prnw_df = prnw_nlp(p_url, p_df)
    print("prnw NLP")
    yahoo_df = yahoo_nlp(y_url, y_df)
    print("yahoo NLP")

    app_df = append_df(yahoo_df,mwatch_df, prnw_df)
    print("Append")

    month_df = row_tags(app_df)
    print("Row tags")

    year_number = str(month_df.year.unique().item())
    month_number = str(month_df.month.unique().item())
    datetime_object = datetime.datetime.strptime(month_number, "%m")
    month_name = datetime_object.strftime("%b").lower()
    file_name = 'nlp_'+month_name+str(year_number)+'.csv'
    month_df.to_csv(file_name,
                   index=False)

toc = time.perf_counter()
#print(f"Completed in {(toc - tic)/60:0.2f} minutes")
print("Completed in " +str(round((toc - tic)/60,2))+ " minutes")
